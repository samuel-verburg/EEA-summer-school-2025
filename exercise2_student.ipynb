{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "61de68b9",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samuel-verburg/EEA-summer-school-2025/blob/main/exercise2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ae01463",
      "metadata": {},
      "source": [
        "#### EEA Summer School 2025: Measurement Methods for Sound Field Analysis, Reconstruction, and Reproduction\n",
        "**Exercise 2: Machine learning and Physics-informed neural networks for sound field reconstruction**\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "Machine learning (ML) has transformed the field of acoustics by enabling data-driven solutions to complex problems such as source localization, speech processing, bioacoustics, sound classification, spatial audio, and room acoustics [Bianco2019]. In particular, the integration of physical principles with ML has been shown to improve the accuracy, efficiency, interpretability, robustness, and generalization capabilities of ML models [Raissi2019].\n",
        "\n",
        "In this notebook, you will learn to:\n",
        "- Create a neural network using PyTorch.\n",
        "- Train a neural network to solve a sound field reconstruction problem.\n",
        "- Incorporate physical constraints to train a PINN.\n",
        "\n",
        "To create and train the networks, we are going to use [`PyTorch`](https://pytorch.org/). We run the models on CPU, but the notebook can be easily modified to run on GPU. Just define your device with something like\n",
        "\n",
        "`device = 'cuda:0' if torch.cuda.is_available() else 'cpu'`\n",
        "\n",
        "and move the tensors to and from the device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59ec53e5",
      "metadata": {
        "id": "59ec53e5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "import torch as torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77d97749",
      "metadata": {},
      "source": [
        "**Creating a neural network class**\n",
        "\n",
        "The following code defines a fully connected network (FCN) class using PyTorch's `nn.Module`, which is the base class for all neural networks in PyTorch. In the fully connected architecture, each neuron in one layer is connected to every neuron in the next layer.\n",
        "\n",
        "<img src=\"FCN.png\" alt=\"Fully Connected Network architecture\" width=\"400\"/>\n",
        "\n",
        "Fig. from [Tonello2019]\n",
        "\n",
        "The inputs to create a network are:\n",
        "- `n_in`: Number of input features.\n",
        "- `n_out`: Number of output features.\n",
        "- `n_units`: Number of hidden units per layer.\n",
        "- `n_layers`: Total number of layers (including input and output layers).\n",
        "\n",
        "We will use *sines* as the activation function. Using sines is beneficial for tasks involving learning high-frequency functions or representing complex signals because they are periodic, smooth, and infinitely differentiable. This allows the network to model continuous signals more effectively than traditional activations like ReLU or tanh. The sine function is not a standard PyTorch activation, so we need to define it first.\n",
        "\n",
        "Then we define the different layers:\n",
        "\n",
        "- The first layer is a linear transformation from `n_in` to `n_units`, followed by the sine activation.\n",
        "- Then comes a sequence of `n_layers-1` hidden layers, each with a linear transformation from `n_units` to `n_units`, followed by the sine activation.\n",
        "- The final output layer is a linear mapping from `n_units` to `n_out`.\n",
        "\n",
        "Finally, the function `forward` describes how an input `r` flows through the network:\n",
        "- Through the first layer (`fcs`).\n",
        "- Through the hidden layers (`fch`).\n",
        "- Through the output layer (`fce`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acd7005c",
      "metadata": {
        "id": "acd7005c"
      },
      "outputs": [],
      "source": [
        "class Sin(nn.Module):\n",
        "    \"\"\"Sin activation function\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self, x):\n",
        "        return torch.sin(x)\n",
        "\n",
        "class FCN(nn.Module):\n",
        "    \"\"\"Fully Connected Network.\"\"\"\n",
        "    def __init__(self, n_in, n_out, n_units, n_layers):\n",
        "        super().__init__()\n",
        "        activation = Sin\n",
        "        self.fcs = nn.Sequential(*[\n",
        "                        nn.Linear(n_in, n_units),\n",
        "                        activation()])\n",
        "        self.fch = nn.Sequential(*[\n",
        "                        nn.Sequential(*[\n",
        "                        nn.Linear(n_units, n_units),\n",
        "                        activation()]) for _ in range(n_layers-1)])\n",
        "        self.fce = nn.Linear(n_units, n_out)\n",
        "    def forward(self, r):\n",
        "        r = self.fcs(r)\n",
        "        r = self.fch(r)\n",
        "        r = self.fce(r)\n",
        "        return r"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7966c24f",
      "metadata": {},
      "source": [
        "**Generate reference and observed data**\n",
        "\n",
        "Now we are ready to generate some data. For simplicity, we will work with a 1D+1 problem, i.e., a problem with one spatial dimension $r$ and time $t$. This could be useful, for example, if we want to study the pressure radiated by a source in the radial direction as a function of time.\n",
        "\n",
        "The pressure field we consider is the one generated by a simple point source:\n",
        "\n",
        "$$\n",
        "p(r,t) = \\frac{\\text{Re}[\\text{e}^{\\text{j}(\\omega t - kr)}]}{4 \\pi r} = \\frac{\\cos(\\omega t - kr)}{4 \\pi r}\n",
        "$$\n",
        "\n",
        "We define points uniformly sampled over $r$ and $t$, and compute the field at these points. This will be the reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75907dd5",
      "metadata": {
        "id": "75907dd5"
      },
      "outputs": [],
      "source": [
        "# Define parameters\n",
        "c = 343.0\n",
        "f = 1000.0\n",
        "k = 2 * np.pi * f / c\n",
        "L = 1.0\n",
        "T = 1e-3\n",
        "\n",
        "r = np.linspace(0.1, L, 50)\n",
        "t = np.linspace(0, T, 50)\n",
        "r_grid, t_grid = np.meshgrid(r, t)\n",
        "rt = np.array([r_grid.flatten(), t_grid.flatten()]).T \n",
        "\n",
        "p_ref = np.exp(1j * (2 * np.pi * f * rt[:, 1] - k * rt[:, 0])) / (4 * np.pi * rt[:, 0])\n",
        "p_ref = p_ref.real.reshape(r_grid.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb17897c",
      "metadata": {},
      "source": [
        "To simulate measurements, we take uniformly spaced samples from the original `r`. The factor `skip` determines the step size for subsampling the spatial dimension. For example, `skip = 7` means that we take every 7th value of `r` to simulate sparse measurements, resulting in a total of 8 measurements.\n",
        "\n",
        "To increase the difficulty of the problem, a significant amount of noise is added to the observed data.\n",
        "\n",
        "Finally, we create an animation of the reference and observed data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc27e67e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "fc27e67e",
        "outputId": "aea02ca0-7e0b-4ddb-d9a6-5b43d8df4e30"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42) # For reproducibility\n",
        "torch.manual_seed(42) # For reproducibility\n",
        "\n",
        "skip = 7\n",
        "p_obs = p_ref[:, 0::skip].copy()\n",
        "r_obs = r[0::skip]\n",
        "rt_obs = np.array([r_grid[:, 0::skip].flatten(), t_grid[:, 0::skip].flatten()]).T\n",
        "\n",
        "# Add noise to observations\n",
        "p_obs += 0.2 * np.random.randn(p_obs.shape[0],p_obs.shape[1]) * np.sqrt(np.mean(np.abs(p_obs)**2))\n",
        "\n",
        "# Make an animation\n",
        "fig = plt.figure(figsize=(6, 4))\n",
        "line, = plt.plot(r, p_ref[0, :], label='Reference')\n",
        "plt.ylim(-1, 1)\n",
        "plt.xlabel('r (m)')\n",
        "plt.ylabel('Pressure')\n",
        "\n",
        "scatter = plt.scatter(r_obs, p_obs[0, :], color='red', label='Observations')\n",
        "plt.legend()\n",
        "\n",
        "def update(frame):\n",
        "    line.set_ydata(p_ref[frame, :])\n",
        "    scatter.set_offsets(np.c_[r_obs, p_obs[frame, :]])\n",
        "    plt.title(f\"Time: {t_grid[frame, 0]:.5f} s\")\n",
        "    return line, scatter\n",
        "\n",
        "ani = FuncAnimation(fig, update, frames=p_ref.shape[0], interval=100, blit=True)\n",
        "html = HTML(ani.to_jshtml())\n",
        "display(html)\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2237ddbd",
      "metadata": {},
      "source": [
        "**Define the neural network**\n",
        "\n",
        "We are going to use a neural network to learn the sound field from noisy observations. First, we will train the network *without* physical constraints. In the next section, we will integrate physical knowledge into the training and compare the results.\n",
        "\n",
        "To work with PyTorch, we first need to convert the NumPy arrays into PyTorch tensors. A PyTorch tensor is a multi-dimensional array used for storing and manipulating data in deep learning models, with support for automatic differentiation and GPU acceleration. As mentioned in the introduction, we will only use the CPU in this example, but we still need to use tensors. Since we need to compute gradients with respect to the input and data tensors, we set `requires_grad_(True)`. This tells PyTorch to track all operations on those tensors.\n",
        "\n",
        "We want to use the neural network to reconstruct the sound field from the observations. Therefore, we define a network with two inputs, $r$ and $t$, and one output, $p$. For simplicity and to speed up training, we define a small number of layers and a small number of units per layer. Note that more complex signals in higher dimensions (e.g., sound fields in 2D and 3D, room impulse responses, or complex radiation problems) will require larger networks.\n",
        "\n",
        "Before starting training, we need to choose a loss function. One of the most common losses is the mean squared error (MSE), for which the loss function looks like\n",
        "$$\n",
        "\\mathcal{L}_\\text{obs}(\\theta) = \\frac{1}{n_\\text{obs}} \\sum_{i=0}^{n_\\text{obs}-1} (p_\\theta(r_i,t_i) - p_\\text{obs}(r_i,t_i))^2\n",
        "$$\n",
        "where $p_\\theta(r_i,t_i)$ is the network output for inputs $r_i,t_i$. The subscript indicates that the output is parameterized by the network parameters $\\theta$. This loss function is what the optimizer will try to minimize during training. In gradient-based optimization, we tweak the parameters $\\theta$ in the direction that minimizes the loss function. [3Blue1Brown has a series of videos](https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=2Bx-VUrRQldL3ahC) that cover the basics of neural network training, gradient descent, and backpropagation.\n",
        "\n",
        "For the optimizer, we choose [ADAM](https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html), which is also very common."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "202c0474",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "202c0474",
        "outputId": "38d0e172-1ba5-4905-e6b2-bc984f60c867"
      },
      "outputs": [],
      "source": [
        "rt_obs = torch.tensor(rt_obs, dtype=torch.float32).requires_grad_(True)\n",
        "p_obs = torch.tensor(p_obs.reshape(-1,1), dtype=torch.float32).requires_grad_(True)\n",
        "rt = torch.tensor(rt, dtype=torch.float32).requires_grad_(True)\n",
        "\n",
        "n_in = 2\n",
        "n_out = 1\n",
        "n_hidden = 32\n",
        "n_layers = 3\n",
        "model = FCN(n_in, n_out, n_hidden, n_layers)\n",
        "\n",
        "n_epochs = int(4e4)\n",
        "mse_loss = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6310db93",
      "metadata": {},
      "source": [
        "**Train the neural network**\n",
        "\n",
        "We are ready to start training.  \n",
        "Let's break down the following code cell:\n",
        "\n",
        "- **Loop over epochs**: `for i in tqdm(range(n_epochs)):` iterates over the number of training epochs. `tqdm` provides a progress bar for visual feedback.\n",
        "\n",
        "- **Zero the gradients**: `optimizer.zero_grad()` clears old gradients from the previous step. This is necessary because PyTorch accumulates gradients by default.\n",
        "\n",
        "- **Forward pass**: `p = model(rt_obs)` feeds the input `rt_obs` through the network to get the predictions `p`.\n",
        "\n",
        "- **Compute the loss**: `loss = mse_loss(p, p_obs)` calculates the mean squared error between predictions `p` and target values `p_obs`.\n",
        "\n",
        "- **Backward pass**: `loss.backward()` computes gradients of the loss with respect to model parameters using backpropagation.\n",
        "\n",
        "- **Update parameters**: `optimizer.step()` updates model parameters using the computed gradients.\n",
        "\n",
        "- **Track loss history**: Initializes `loss_history` on the first iteration and appends the current loss value to the history list. This will serve later to check the training dynamics.\n",
        "\n",
        "- **Print progress**: Every 5000 epochs, prints the current loss to monitor training progress.\n",
        "\n",
        "You can stop and resume training at any time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fc2f112",
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in tqdm(range(n_epochs)):\n",
        "    optimizer.zero_grad()\n",
        "    p = model(rt_obs)\n",
        "    loss = mse_loss(p, p_obs)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i == 0:\n",
        "        loss_history = []\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "    if i % 5000 == 0:\n",
        "      print('Loss:', loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2f5180c",
      "metadata": {},
      "source": [
        "**ex2.0**: \n",
        "\n",
        "Plot and analyze the training loss history. How does the loss evolve as the network is trained?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7f2ab6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "c7f2ab6b",
        "outputId": "20aa3967-8a50-46b1-fd6c-d930229bed53"
      },
      "outputs": [],
      "source": [
        "# Here comes your code:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "698d5fbb",
      "metadata": {},
      "source": [
        "**ex2.1**:\n",
        "\n",
        "Make an animation of the pressure predicted over the whole $r$ together with the reference pressure. What can you observe?\n",
        "\n",
        "To get the estimated pressure, you need to input the tensor `rt` to the network. For plotting, you need to detach it from the computational graph and convert it to a NumPy array. You should reshape the array to make it easier to plot. Altogether, it would look like:\n",
        "\n",
        "`p_rec = model(rt).detach().numpy()`\n",
        "\n",
        "`p_rec = p_rec.reshape(r_grid.shape)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3872591",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "c3872591",
        "outputId": "5ce1f76d-366d-4b48-d637-d304b85bf998"
      },
      "outputs": [],
      "source": [
        "# Here comes your code:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1adc9785",
      "metadata": {},
      "source": [
        "**Physics-informed neural networks**\n",
        "\n",
        "You can probably see that the prediction is not very good. Indeed, the problem is quite hard, since we have very few data points and the noise level is high. A network trained only on this data is likely to fail at reconstructing the sound field.\n",
        "\n",
        "Physics-informed neural networks (PINNs) incorporate physical laws, typically expressed as partial differential equations (PDEs), into the training process. This constrains the network output to not only fit the observed data but also to satisfy the underlying physics of the problem. Since we are dealing with sound fields, we know that the solution must satisfy the wave equation,\n",
        "$$\n",
        "\\frac{\\partial^2 p}{\\partial r^2} - \\frac{1}{c^2} \\frac{\\partial^2 p}{\\partial t^2} = 0.\n",
        "$$\n",
        "\n",
        "One can see that to compute the wave equation, we need to take the derivatives of the pressure with respect to $r$ and $t$ twice. The key to PINNs is that such derivatives can be computed automatically using *automatic differentiation*, which allows for exact and efficient differentiation of the neural network's output with respect to its inputs [Baydin2018, Moseley2020]. \n",
        "\n",
        "To train the PINN, the loss function is modified to include the PDE constraint:\n",
        "$$\n",
        "\\mathcal{L} = \\mathcal{L}_\\text{obs} + \\alpha\\mathcal{L}_\\text{pde}\n",
        "$$\n",
        "where\n",
        "$$\n",
        "\\mathcal{L}_\\text{pde}(\\theta) = \\frac{1}{n_\\text{colloc}} \\sum_{i=0}^{n_\\text{colloc}-1} \\left(\\frac{\\partial^2 p_\\theta (r_i,t_i)}{\\partial r^2} - \\frac{1}{c^2} \\frac{\\partial^2 p_\\theta (r_i,t_i)}{\\partial t^2}\\right)^2.\n",
        "$$\n",
        "We can see that $\\mathcal{L}_\\text{pde}(\\theta)$ corresponds to the PDE residual. \n",
        "\n",
        "The points $r_i,t_i$, for $i=0, \\dots, n_\\text{colloc}$, are sampled over the entire spatial-temporal domain—not only at the observation points. In this way, the PDE constraint is applied over the whole domain, making it possible to predict the pressure at 'unseen' locations. In fact, we are improving the generalization capabilities of the network. \n",
        "\n",
        "To compute the PDE partial derivatives, we use the function [`torch.autograd.grad`](https://docs.pytorch.org/docs/stable/generated/torch.autograd.grad.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4106ef03",
      "metadata": {
        "id": "4106ef03"
      },
      "outputs": [],
      "source": [
        "def pde_residual(p, rt, c):\n",
        "    p_rt = torch.autograd.grad(p, rt, torch.ones_like(p), create_graph=True)[0] # dp/dr and dp/dt\n",
        "    p_rr = torch.autograd.grad(p_rt[:,0], rt, torch.ones_like(p_rt[:,0]), create_graph=True)[0][:,0:1] # d^2p/dr^2\n",
        "    p_tt = torch.autograd.grad(p_rt[:,1], rt, torch.ones_like(p_rt[:,1]), create_graph=True)[0][:,1:2] # d^2p/dt^2\n",
        "    pde_res = p_rr - p_tt / c**2\n",
        "    return pde_res\n",
        "\n",
        "alpha = 1e-3\n",
        "model_pinn = FCN(n_in, n_out, n_hidden, n_layers)\n",
        "optimizer = torch.optim.Adam(model_pinn.parameters(), lr=1e-3)\n",
        "n_epochs = int(4e4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76495145",
      "metadata": {},
      "source": [
        "Below you can see that training the PINN is very similar to training the previous model. The only difference is that now the loss is composed by two terms: ` loss_obs` and `loss_pde`. \n",
        "\n",
        "The relative weight is controled by the hyperparameter `alpha`. In fact, properly weihting these terms is crucial to the sucess of the training. This can be seen as a 'weakness' of PINNs, since the results can be very sensitive to the choise of `alpha`. Different methods to automatically find the relative weight have been developed, but to keep this example simple, we are just going to make `alpha = 1e-3` manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0a3efd4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0a3efd4",
        "outputId": "48c99436-f8b5-4f33-8206-f13d7813972c"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i in tqdm(range(n_epochs)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    p = model_pinn(rt_obs)\n",
        "    loss_obs = mse_loss(p, p_obs)\n",
        "\n",
        "    r_colloc = torch.randn_like(rt_obs, dtype=torch.float32, requires_grad=True) * torch.tensor([L, T])\n",
        "    p = model_pinn(r_colloc)\n",
        "    loss_pde = mse_loss(pde_residual(p, r_colloc, c), torch.zeros_like(p))\n",
        "\n",
        "    loss = loss_obs + loss_pde*alpha\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if i == 0:\n",
        "        loss_history = []\n",
        "        loss_obs_history = []\n",
        "        loss_pde_history = []\n",
        "\n",
        "    loss_history.append(loss.item())\n",
        "    loss_obs_history.append(loss_obs.item())\n",
        "    loss_pde_history.append(loss_pde.item())\n",
        "\n",
        "    if i % 5000 == 0:\n",
        "      print(loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d7081ff",
      "metadata": {},
      "source": [
        "**ex2.2**: \n",
        "\n",
        "Plot and analyze the training loss history. How do the different terms of the loss evolve as the network is trained?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d347e48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "2d347e48",
        "outputId": "cb279898-f344-437d-f987-6d1ccaa6878d"
      },
      "outputs": [],
      "source": [
        "# Here comes your code:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6216c0b8",
      "metadata": {},
      "source": [
        "**ex2.3**:\n",
        "\n",
        "Make an animation of the pressure predicted by the PINN over the whole $r$ together with the reference pressure. How do the results compare with the network trained without physics? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c9a406",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "04c9a406",
        "outputId": "1950535d-b8c0-4e8f-dcd6-5973e3530d4e"
      },
      "outputs": [],
      "source": [
        "# Here comes your code:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a2abcc5",
      "metadata": {
        "id": "2a2abcc5"
      },
      "source": [
        "**Bonus exercises / deliverables**\n",
        "\n",
        "- You can try to solve a 2D reconstruction problem. You will need to generate data in 2D+1, and the PDE residual should also be modified to account for the extra dimension. Note that this problem will require larger networks, longer training times, and more observed data.\n",
        "\n",
        "- An interesting application of PINNs is to estimate parameters of the PDE (e.g., the speed of sound $c$) together with the pressure field. For that, you will need to make $c$ a learnable parameter and optimize it together with the network parameters. It could look something like:\n",
        "\n",
        "`c0 = 340  # initialize value` \n",
        "\n",
        "`c = torch.nn.Parameter(c0, requires_grad=True)`\n",
        "\n",
        "`optimizer = torch.optim.Adam([{'params': model.parameters(), \"lr\": 1e-3}, {'params': c, \"lr\": 1e-3}])`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "024d0b63",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here comes your code:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c250b68d",
      "metadata": {},
      "source": [
        "**References**\n",
        "\n",
        "- [Bianco2019] Bianco, M. J., Gerstoft, P., Traer, J., Ozanich, E., Roch, M. A., Gannot, S., & Deledalle, C. A. (2019). Machine learning in acoustics: Theory and applications. The Journal of the Acoustical Society of America, 146(5), 3590-3628.\n",
        "- [Raissi2019] Raissi, M., Perdikaris, P., & Karniadakis, G. E. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378, 686-707.\n",
        "- [Moseley2020] Moseley, B., Markham, A., & Nissen-Meyer, T. (2020). Solving the wave equation with physics-informed deep learning. arXiv preprint arXiv:2006.11894.\n",
        "- [Tonello2019] Tonello, A., Letizia, N., Righini, D., and Marcuzzi, F. (2019). Machine Learning Tips and Tricks for Power Line Communications. IEEE Access. 7. 1-1\n",
        "- [Baydin2018] Baydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2018). Automatic differentiation in machine learning: a survey. Journal of machine learning research, 18(153), 1-43."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "eea-summer-school-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
